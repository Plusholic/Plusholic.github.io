---
layout: single
title:  "Model Free Control"
categories : 강화학습
tags : [python, reinforcement learning, 강화학습, Model Free control, On policy, SARSA, off policy, Q learning]
toc : true
author_profile : false
sidebar : 
    nav : "docs"
use_math: true


---





# **4**   **Model Free control**

On policy는 behavior policy와 improvement policy를 같은 것을 사용하고, off policy -> behavior policy와 improvement policy를 다른 것을 사용한다.

 

## **4.1**   **On Policy**

예를 들어 Policy evaluation을 MC policy evaluation을 사용하고, Policy improvemnet를 Greedy를 사용한다면 가능할까?

MC는 model free 상황에서도 evaluation이 가능하지만 MC는 MDP를 안다는 가정이기 때문에 greedy policy improvement를 사용할 수 없다. 다음 state를 모르면 greedy policy를 만들 수 없기 때문이다.
$$
\pi' = \mathop{\arg \max}\limits_{a \in A} \ R_{s}^{a} + P_{ss'}^{a}V(s')
$$


Greedy policy는 MDP의 model을 알고 있는 상태에서만 $V(s)$ 를 업데이트 할 수 있다.

하지만 $Q(s, a)$ 의 경우에는 $s$ 에서 $a$ 를 해보고 나 온 값들의 평균이기 때문에, model-free 상황에서도 업데이트를 할 수 있다.
$$
\pi' = \mathop{\arg \max}\limits_{a \in A} \ Q(s, a)
$$


$s$ 에서 할 수 있는 action들이 몇 가지인지는 알고 있기 때문에, 할 수 있는 action 중 $q$ 값이 가장 높은 것을 policy로 하면 된다. 즉, Policy improvement는 $v$ 로는 할 수 없지만 $q$ 로는 할 수 있다.

 

### **4.1.1** $\epsilon - greedy$

Greedy 하게 움직이면 충분히 많은 state를 방문할 수 없다. 그래서 $\epsilon - greedy$  방법론이 제안되었다. 이 방법은 $\epsilon$ 의 확률로 다른 action을 선택하고, $1 - \epsilon$ 의 확률로 greedy action을 선택한다.
$$
\pi(a|s) = 
\begin{cases}
\frac{\epsilon}{m}, & \mbox{if } \ a^{*} = \mathop{\arg \max}\limits_{a \in A} \ Q(s, a) \\
\frac{\epsilon}{m}, & \mbox{otherwise}
\end{cases}
$$


$\epsilon - greedy$ 를 해도 value function이 개선되어 improvement가 가능하다는 것이 증명 되어있다.

$\epsilon - greedy$ 는 every episode마다 policy evaluation, policy improvement가 가능하다.

그리고 optimal을 달성했음에도 작은 확률로 다른 action을 하는 것을 방지하기 위해서 Greedy in the Limit with Infinite Exploration(GILE)를 정의한다. 이것은 시간이 지날수록 $\epsilon$ 을 더 작게 만들어준다. GILE MC control은 optimal action value function으로 수렴한다. $Q(s, a) \rightarrow q_{\*}(s, a)$

 

### **4.1.2** **SARSA**

지금까지는 MC control이었는데, MC를 TD로 대체할 수도 있다. TD는 MC에 비해 lower variance이며 episode가 끝나지 않아도 되는 장점이 있기 때문이다.

SARSA라고 불리는 알고리즘인데, 한 스텝 가서 q를 업데이트 하고, $\epsilon - greedy$ 로 다음 스텝으로 업데이트 하고 다음 스텝에서 다시 q를 업데이트 하는 방법으로 매 time step마다 업데이트가 가능하다.
$$
Q(S, A) \leftarrow Q(S', A') + \alpha(R + \gamma Q(S', A') - Q(S, A))
$$


SARSA는 아래의 조건에서 optimal action-value function으로 수렴한다.

$\pi_{t}(a|s)$ 는 GILE policy sequence이고, step size $a_{t}$ 는 Robbins-Monro sequence이다.
$$
\sum_{t=1}^{\infty} a_{t} = \infty \\
\sum_{t=1}^{\infty} a_{t}^{2} < \infty
$$


이것은 충분히 큰 $q$ 까지 업데이트가 가능하다는 의미와 q-value로 수렴하는 값이 점점 작아진다는 의미이다.

 

### **4.1.3** **n-step SARSA**

n-step SARSA는 n-step TD와 유사하다. TD target 부분에 $q_{t}$ 를 넣어서 쓸 수 있다.
$$
\begin{align}
n &= 1 \ \ (SARSA) \ \ q_{t}^{(1)} = R_{t+1} + \gamma Q(S_{t+1}) \\
n &= 2 \ \ q_{t}^{(2)} = R_{t+1} + \gamma R_{t+2} + \gamma^2Q(S_{t+2}) \\
\cdots \\
n &= \infty \ \ (MC) \ \ q_{t}^{(\infty)} = R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{T-1}R_{T}
\end{align}
$$




### **4.1.4** **Forward and Backward View Sarsa(**\lambda**)**

Forward View Sarsa(\lambda) 도 Forward View TD와 유사하다.
$$
q_{t}^{\lambda} = (1-\lambda)\sum_{n=1}^{\infty} \lambda^{n-1} q_{t}^{(n)} \\
Q(S_{t}) \leftarrow Q(S_{t}) + \alpha (q_{t}^{\lambda} - Q(S_{t}))
$$


Backward View 역시 동일하다.
$$
\begin{align}
& E_{0}(s, a) = 0 \\
& E_{t}(s, a) = \gamma \lambda E_{t-1}(s, a) + 1 \ (S_{t}=s, A_{t}=a) \\
& \delta_{t} = R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_{t}, A_{t}) \\ 
& Q(s, a) \leftarrow Q(s, a) + \alpha \delta_{t} E_{t} (s, a)
\end{align}
$$


## **4.2**   **Off policy**

Off policy는 탐험적인 행동을 하면서 동시에 optimal을 찾을 수 있다. On policy에서는 경험을 쌓고나면 policy가 조금 달라지는데, 이거로 다시 경험을 쌓아야 의미가 있다. 그래서 경험을 쌓고나면 policy를 버린다. 하지만 off policy에서는 re-use가 가능하다.

예를 들면 일반적인 주사위와, 눈금이 나올 확률이 조금씩 다른 주사위가 있다고 했을 때, 확률이 다른 주사위를 던지면서 일반적인 주사위를 던질 때와 같은 policy를 취해도 optimal을 달성할 수 있다.

여기서 behavior policy를 $\mu$, target policy를 $\pi$ 라고 한다.
$$
G^{\frac{\pi}{\mu}} = \frac{\pi(A_{t} | S_{t}) \pi(A_{t+1} | S_{t+1})}{\mu(A_{t} | S_{t})\mu({A_{t+1}|S_{t+1})}} \cdots \frac{\pi(A_{T} | S_{T})}{\mu(A_{T} | S_{T})} G_{T}
$$


모든 조건부 확률의 비율을 곱해주면 다른 정책을 취하더라도 correction을 시킬 수 있다. 다른 정책을 따랐 음에도 이 policy를 따랐을 때의 return을 구할 수 있지만 variance가 너무 커서 실제로 사용할 수는 없는 방법이다.

 

### **4.2.1** **Q learning**

하지만 위의 식에서 MC 대신 TD를 사용하면 variance를 줄일 수 있다.
$$
V(S_{t}) \leftarrow V(S_{t}) +
\alpha \left(\frac{\pi(A_{t} | S_{t})}{\mu(A_{t} | S_{t})} (R_{t+1} + \gamma V(S_{t+1})-V(S_{t}) \right)
$$


이제 off policy에서 action value $Q(S, A)$를 학습하는 것을 고려한다. 더 이상 sampling은 중요하지 않다.
$$
Q(S_{t}, A_{t}) \leftarrow Q(S_{t}, A_{t}) + \alpha(R_{t+1} + \gamma Q(S_{t+1},A')-Q(S_{t},A_{t}))
$$


$S$ 에서 $A$ 를 할 때의 value를 한 스텝 가보고 그 step에서 추측한 거로 업데이트 하는게 TD learning이다. 그 추측치는 behavior policy를 안 써도 된다. 즉 off policy가 가능하다.

Behavior policy와 target policy를 모두 improve 시킬 수 없을까? Target policy $\pi$ 는 greedy이고, behavior policy $\mu$ 는 $\epsilon - greedy$ 로 잡는다.

그러면 Q-learning target은 다음과 같다.
$$
\begin{align}
R_{t+1} + \gamma Q(S_{t+1}, A') & = R_{t+1} + \gamma Q (S_{t+1}, \mathop{\arg \max}\limits_{a \in A} \ Q(S_{t+1}, a')) \\
& = R_{t+1} + \gamma \max_{a'} Q(S_{t+1}, a')
\end{align}
$$
Q-learning control은 optimal action value function으로 수렴한다 
$$
Q(s, a) \rightarrow q_{*}(s, a)
$$
