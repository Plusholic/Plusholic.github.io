# **1**   **Markov decision process** 

Markov decision process(MDP)는 강화학습에서 관측가능한 모든 environment를 뜻하며, 거의 대부분의 RL 문제를 MDP로 표현할 수 있다. MDP를 이해하기 위해서는 먼저 Markov Property를 알아야 한다.

 

## **1.1**   **Markov Property**

State _S_<sub>_t_</sub>  가 Markov라는 것은 _P [ S<sub>t+1</sub> | S<sub>t</sub> ]_ = _P [ S<sub>t+1</sub> | S<sub>1</sub>, S<sub>2</sub> , ... , S<sub>t</sub> ]_ 와 동치이고, 이것을 Markov Property라 한다.



이것의 의미는 State는 미래에 대한 충분한 통계적 표현이기 때문에, State가 history의 모든 정보를 가지고 있어서 state만 필요하고 history는 버릴 수 있다는 뜻이다.

Markov state s와 다음 state s’에 대해서 state transition probability는 다음과 같이 정의된다.

![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image006.png)

그리고 State transition matrix는 모든 stats s로부터 모든 다음 state s’로의 transition probability로 정의된다.

![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image008.png)

여기서 ![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image010.png)는 state 1에서 n으로 전이될 확률이다. 위의 행렬의 원소의 합은 1이다.

 

random variable과 random process의 차이를 알아야 하는데, random variable은 시간을 고려하지 않은 것이고, random process는 시간을 고려한 것이다. 즉 Sampling을 할 수 있고, Markov process는 state의 sequence가 다양하게 존재할 수 있다. 그래서 Markov process를 memoryless random process라고 부른다. 어느 경로를 통해서 여기까지 왔는지에는 관계없이 현재 state에 의존해서 미래가 정의되기 때문이다.

 

## **1.2**   **A Markov Process(or Markov Chain)**

A Markov Process(or Markov Chain) 은 ![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image012.png) 로 정의된다. 즉 Markov process는 State들의 집합과 transition matrix만 알면 완전히 표현이 가능하다. 여기서 ![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image014.png) 는 유한개의 state의 집합이고 ![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image016.png) 는 ![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image018.png) 를 만족시키는 state transition probability matrix이다.

 

## **1.3**   **Markov Reward Process(MRP)**

Markov reward process는 value가 있는 Markov chain이다. Markov reward process는 ![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image020.png)로 표현되는데, 이 4개로 Markov reward process의 정보가 갖춰진다.

![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image014.png)는 유한개의 state의 집합이고,

![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image016.png)는 ![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image018.png) 를 만족시키는 state transition probability matrix이다.

![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image025.png)은 reward function, ![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image027.png)이고, 

![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image029.png)는 0과 1 사이의 discount factor이다.

 

### **1.3.1** **Return**

강화학습은 return을 maximize하기 위한 목적을 가지고 있다. return이란 reward를 시간에 따라 discount해서 다 더한 것이다.

![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image031.png)

![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image029.png)은 미래 reward의 현재 가치를 의미한다. ![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image034.png) time step이 지난 후의 reward는 ![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image036.png)이다. 여기서 ![img](file:////Users/jeonjunhwi/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image029.png)가 0이라면 위 식의 뒤쪽 항들이 0이 되어 바로 다음의 reward에만 집중한다. 그리고 ![img](file:////Users/jeonjunhwi/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image029.png)가 1이라면 장기적인 reward 까지도 현재와 동일하게 중요하게 고려된다.

Discount factor ![img](file:////Users/jeonjunhwi/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image029.png)를 붙이는 이유는 먼저 수학적으로 편리하기 때문이고, infinite returns을 피할 수 있기 때문이다. 그리고 미래에 대한 불확실성을 표현할 수 있고, 동물과 사람은 즉각적인 리워드를 선호하는 경향이 있다는 측면을 반영할 수 있다. 어떻게 에피소드를 샘플링 하던지 terminal이 보장되어 있다면 ![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image041.png)이 되는 경우도 존재할 수 있다.

 

### **1.3.2** **Value function**

Value function은 state s의 장기적인 value를 의미한다. Markov Reward Process의 state value function v(s)는 state s부터 시작할때의 return의 기댓값이 된다. 즉 s에서 시작할 때의 모든 미래에 받을 수 있는 reward의 평균이다.

![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image043.png)

 

### **1.3.3** **Bellman Equation of MRPs**

Value function은 두개의 파트로 나눌 수 있는데, 즉각적인 reward인 ![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image045.png)과 discounted value인 ![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image047.png)이다. 수식은 아래와 같다.

![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image043.png)

![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image050.png)

![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image052.png)

![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image054.png)

![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image056.png)

다음 스텝에서의 reward + 다음 state에서의 value로 볼 수 있다.

![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image058.png)

![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image060.png)

현재 state에 도달했을 때의 reward를 받고, 다음 state로 갈 확률과 그 때의 value function을 곱해서 합해준다.

 

Bellman Equation은 아래와 같이 행렬을 통해서도 표현할 수 있다.

![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image062.png)

![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image064.png)

Bellman equation은 linear equation이므로 direct 하게 풀 수 있다.

![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image062.png)

![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image067.png)

![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image069.png)

 

*MRP는 Markov Process에서 reward와* ![img](file:////Users/jeonjunhwi/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image029.png)*가 더해진 것이다**. MDP(Markov Decision Process)는 MRP에서 action이 더해진 것이다.*

 

## **1.4**   **Markov Decision Process(MDP)**

Markov Decision Process는 ![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image072.png) 으로 정의된다.

![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image074.png) 는 유한개의 action의 집합을 의미한다.

![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image016.png) 는 state transition probability matix이며 ![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image077.png) 이다.

![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image079.png) 은 reward function이며, ![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image081.png) 이다.

Policy ![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image083.png) 는 주어진 state에서 action을 할 확률분포이다.

Policy는 agent의 행위에 의해 전적으로 정의된다.

MDP의 policy는 현재 state에 의해서만 정의된다.

 

### **1.4.1**  **Value function**

MDP의 value function은 State value function과 Actin value function으로 나뉜다.

State value function ![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image085.png) 는 s에서 시작해서 policy ![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image083.png)를 따랐을 때 return의 기댓값이고, Action value function ![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image088.png) 은 state s에서 시작해서 action a를 취하고, 그 후에 policy ![img](file:////Users/jeonjunhwi/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image083.png)를 따랐을 때의 기댓값이다.

State value function은 즉각적은 reward와 discount value 부분으로 나눌 수 있다.

![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image091.png)

이것은 일단 한 스텝을 가서 reward를 받고, 그 다음 state부터 ![img](file:////Users/jeonjunhwi/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image083.png)를 따랐을 때의 value function이다.

![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image094.png)

Action value function은 s에서 a를해서 reward를 받고, 다음 state부터 ![img](file:////Users/jeonjunhwi/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image083.png)를 따라 action을 할 때의 기댓값으로 정의한다.

 

### **1.4.2** **Optimal value function**

어떤 policy가 있을 때, value function을 구하는 것은 bellman expectation equation을 풀어서 얻을 수 있다. Optimal을 풀 때는 bellman optimality equation이 필요하다.

The optimal state value function ![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image097.png) 로 정의하고, 가능한 모든 policy에 대해서 value의 값이 최대가 되는 함수이다. 

The optimal action-value function ![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image099.png) 역시 위와 유사하게 정의된다.

Optimal value function은 MDP에서 최고의 성능을 낸다. MDP를 풀었다는 것은 optimal value function을 찾았다는 뜻이다.

 

두 개의 policy를 비교할 수 있어야 optimal을 구할 수 있다.

![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image101.png) ![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image103.png)

모든 state에 대해서 더 좋을 때 크기를 비교할 수 있고, 아래가 성립한다.

![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image105.png)

![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image107.png)

 

### **1.4.3** **Optimal Policy**

Optimal policy는 ![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image109.png)를 maximizing해서 찾을 수 있다.

![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image111.png)

![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image113.png)는 ![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image115.png)에서 어떤 ![img](/Users/jeonjunhwi/문서/Projects/Private_study/github_io/plusholic.github.io/images/2022-08-12-강화학습/clip_image117.png)를 해야 하는지를 알려주기 때문에 당연히 optimal policy일 수밖에 없다. 그래서 ![img](file:////Users/jeonjunhwi/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image113.png)를 안다는 것은 문제를 푼 것과 같다.

 

*Bellman optimality Equation은 식에 max가 들어가기 때문에 nonlinear이다. 위의 방법처럼 직접적으로 풀 수 없기 때문에 approximation을 해야한다. 그 방법들은 Value Iteration, Policy Iteration, Q-learning, Sarsa 등이 있다.*