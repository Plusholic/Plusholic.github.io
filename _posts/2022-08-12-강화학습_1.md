---
layout: single
title:  "강화학습_1"
categories : coding
tags : [python, reinforcement learning, 강화학습, MDP, Markov decision process, Markov Property, Markov Cahin, Markov Reward Process, MRP, Bellman Equation]
toc : true #이걸 추가해서 page의 컨텐츠 순서를 줄 수 있음.
author_profile : false
sidebar : 
    nav : "docs"
use_math: true

---

# **1**   **Markov decision process** 

Markov decision process(MDP)는 강화학습에서 관측가능한 모든 environment를 뜻하며, 거의 대부분의 RL 문제를 MDP로 표현할 수 있다. MDP를 이해하기 위해서는 먼저 Markov Property를 알아야 한다.

 

## 1.1 Markov Property

State $ S_{t} $  가 Markov라는 것은 $ P [ S_{t+1} \| S_{t} ] = P [ S_{t+1} \| S_{1}, S_{2} , ... , S_{t} ] $ 와 동치이고, 이것을 Markov Property라 한다.

이것의 의미는 State는 미래에 대한 충분한 통계적 표현이기 때문에, State가 history의 모든 정보를 가지고 있어서 state만 필요하고 history는 버릴 수 있다는 뜻이다.

Markov state s와 다음 state _s'_ 에 대해서 state transition probability는 다음과 같이 정의된다.   


$$
\\
P_{ss'} = P [ S_{t+1} = s' | S_{t} = s ]
\\
$$


그리고 State transition matrix는 모든 stats s로부터 모든 다음 state s’로의 transition probability로 정의된다.   


$$
\\
P =\begin{pmatrix}
	P_{1,1} & P_{1,2} & \cdots & P_{1,n} \\
  P_{2,1} & P_{2,2} & \cdots & P_{2,n} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  P_{n,1} & P_{n,2} & \cdots & P_{n,n}
 \end{pmatrix}
\\
$$


여기서 $P_{n,1}$ 는 state $1$ 에서 $n$ 으로 전이될 확률이다. 위의 행렬의 원소의 합은 $1$이다.  

random variable과 random process의 차이를 알아야 하는데, random variable은 시간을 고려하지 않은 것이고, random process는 시간을 고려한 것이다. 즉 Sampling을 할 수 있고, Markov process는 state의 sequence가 다양하게 존재할 수 있다. 그래서 Markov process를 memoryless random process라고 부른다. 어느 경로를 통해서 여기까지 왔는지에는 관계없이 현재 state에 의존해서 미래가 정의되기 때문이다.

 

## **1.2**   **A Markov Process(or Markov Chain)**

A Markov Process(or Markov Chain) 은 $<S, P>$ 로 정의된다. 즉 Markov process는 State들의 집합과 transition matrix만 알면 완전히 표현이 가능하다. 여기서 $S$ 는 유한개의 state의 집합이고 $P$ 는 $P_{ss'} = P [ S_{t+1} = s' \| S_{t} = s ]$ 를 만족시키는 state transition probability matrix이다.

 

## **1.3**   **Markov Reward Process(MRP)**

Markov reward process는 value가 있는 Markov chain이다. Markov reward process는 $<S, P, R, \gamma>$ 로 표현되는데, 이 4개로 Markov reward process의 정보가 갖춰진다.

$S$ 는 유한개의 state의 집합이고, $P$ 는 $P_{ss'} = P [ S_{t+1} = s' \| S_{t} = s ]$ 를 만족시키는 state transition probability matrix이다. $R$ 은 reward function, $R_{s} =  \mathbb{E} [ R_{t+1} \| S_{t} = s ]$이고, $\gamma$ 는 0과 1 사이의 discount factor이다.

 

### **1.3.1** **Return**

강화학습은 return을 maximize하기 위한 목적을 가지고 있다. return이란 reward를 시간에 따라 discount해서 다 더한 것이다.   


$$
\\
G_{t} = R_{t+1} + \gamma R_{t+2} + \cdots = \sum_{0}^{\infty} \gamma^{k} R_{t+k+1}
\\
$$


$\gamma$ 은 미래 reward의 현재 가치를 의미한다. $k+1$ time step이 지난 후의 reward는 $\gamma^{k} R$ 이다. 여기서 $\gamma$ 가 $0$ 이라면 위 식의 뒤쪽 항들이 $0$ 이 되어 바로 다음의 reward에만 집중한다. 그리고 $\gamma$ 가 $1$ 이라면 장기적인 reward 까지도 현재와 동일하게 중요하게 고려된다.

Discount factor $\gamma$ 를 붙이는 이유는 먼저 수학적으로 편리하기 때문이고, infinite returns을 피할 수 있기 때문이다. 그리고 미래에 대한 불확실성을 표현할 수 있고, 동물과 사람은 즉각적인 리워드를 선호하는 경향이 있다는 측면을 반영할 수 있다. 어떻게 에피소드를 샘플링 하던지 terminal이 보장되어 있다면 $\gamma + 1$이 되는 경우도 존재할 수 있다.

 

### **1.3.2** **Value function**

Value function은 state s의 장기적인 value를 의미한다. Markov Reward Process의 state value function $v(s)$ 는 state $s$ 부터 시작할때의 return의 기댓값이 된다. 즉 $s$ 에서 시작할 때의 모든 미래에 받을 수 있는 reward의 평균이다.   


$$
\\
v(s) = \mathbb{E}[G_{t} | S_{t} = s]
\\
$$

###    

### **1.3.3** **Bellman Equation of MRPs**

Value function은 두개의 파트로 나눌 수 있는데, 즉각적인 reward인 $R_{t+1}$과 discounted value인 $\gamma v(S_{t+1})$이다. 수식은 아래와 같다.   


$$
\\
\begin{align}
v(s) &= \mathbb{E}[G_{t} | S_{t} = s ]\\
&=\mathbb{E}[R_{t+1} + \gamma R_{t+2} + \gamma ^2 R_{t+3} + \cdots | S_{t} = s] \\
&=\mathbb{E}[R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \cdots) | S_{t} = s] \\
&=\mathbb{E}[R_{t+1} + \gamma G_{t+1} | S_{t} = s]\\
&=\mathbb{E}[R_{t+1} + \gamma v(S_{t+1} | S_{t} = s)]
\end{align}
\\
$$


다음 스텝에서의 reward + 다음 state에서의 value로 볼 수 있다.   


$$
\\
\begin{align}
v(s) &= \mathbb{E}[R_{t+1} + \gamma v(S_{t+1} | S_{t} = s)] \\
v(s) &= R_{s} + \gamma \sum_{s'\in S}P_{ss'}v(s')
\end{align}
\\
$$


현재 state에 도달했을 때의 reward를 받고, 다음 state로 갈 확률과 그 때의 value function을 곱해서 합해준다. Bellman Equation은 아래와 같이 행렬을 통해서도 표현할 수 있다.   


$$
\\
\begin{align}
v &= R + \gamma Pv \\ \\
\begin{pmatrix}
v(1) \\ \\ v(n)
\end{pmatrix}
&=
\begin{pmatrix}
R_{1} \\ \\ R_{n}
\end{pmatrix}
+\gamma \begin{pmatrix}
P_{11} \ \ \ \ \ \ P_{1n} \\ 
\\
P_{n1} \ \ \ \ \ \ P_{nn}
\end{pmatrix}
\begin{pmatrix}
v(1) \\ \\ v(n)
\end{pmatrix}
\end{align}
\\
$$


Bellman equation은 linear equation이므로 direct 하게 풀 수 있다.   


$$
\\
\begin{align}
v &= R + \gamma Pv \\
(I - \gamma P)v &= R \\
v &= (I - \gamma P)^{-1} R
\end{align}
\\
$$


*MRP는 Markov Process에서 reward와* $\gamma$ 가 더해진 것이다**. MDP(Markov Decision Process)는 MRP에서 action이 더해진 것이다.*

 

## **1.4**   **Markov Decision Process(MDP)**

Markov Decision Process는 $<S, A, P, R, \gamma>$ 으로 정의된다.

$A$ 는 유한개의 action의 집합을 의미한다.

$P$ 는 state transition probability matix이며

$P_{ss'}^{a} = \mathbb{E}[R_{t+1} \| S_{t} = s , A_{t} = a]$ 이다.

$R$ 은 reward function이며,

$R_{s}^{a} = \mathbb{E} [R_{t+1} \| S_{t} = s, A_{t} = a]$ 이다.

Policy $\pi$ 는 주어진 state에서 action을 할 확률분포이다. Policy는 agent의 행위에 의해 전적으로 정의된다. MDP의 policy는 현재 state에 의해서만 정의된다.

 

### **1.4.1**  **Value function**

MDP의 value function은 State value function과 Actin value function으로 나뉜다.

State value function $v_{\pi} (s) = \mathbb{E_{\pi}} [G_{t} \| S_{t} = s]$ 는 $s$ 에서 시작해서 policy $\pi$ 를 따랐을 때 return의 기댓값이고, $q_{\pi}(s,a) = \mathbb{E}[G_{t} \| S_{t} = s, A_{t} = a]$ Action value function $q_{\pi}(s,a) = \mathbb{E}[G_{t} \| S_{t} = s, A_{t} = a]$ 은 state $s$ 에서 시작해서 action $a$ 를 취하고, 그 후에 policy $ \pi $ 를 따랐을 때의 기댓값이다. State value function은 즉각적은 reward와 discount value 부분으로 나눌 수 있다.   


$$
\\
v_{\pi}(s) = \mathbb{E}_{\pi}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_{t} = s]
\\
$$


이것은 일단 한 스텝을 가서 reward를 받고, 그 다음 state부터 $\pi$ 를 따랐을 때의 value function이다.   


$$
\\
q_{\pi}(s, a) = \mathbb{E}_{\pi}[R_{t+1} + \gamma q_{\pi}(S_{t+1}, A_{t+1}) | S_{t} = s, A_{t} = a]
\\
$$


Action value function은 s에서 a를해서 reward를 받고, 다음 state부터 $\pi$ 를 따라 action을 할 때의 기댓값으로 정의한다.

 

### **1.4.2** **Optimal value function**

어떤 policy가 있을 때, value function을 구하는 것은 bellman expectation equation을 풀어서 얻을 수 있다. Optimal을 풀 때는 bellman optimality equation이 필요하다.

The optimal state value function $v_{\*}(s) = \max\limits_{\pi} v_{\pi}(s)$ 로 정의하고, 가능한 모든 policy에 대해서 value의 값이 최대가 되는 함수이다. 

The optimal action-value function $q_{\*}(s, a) = \max_\limits{\pi} q_{\pi}(s, a)$ 역시 위와 유사하게 정의된다.

Optimal value function은 MDP에서 최고의 성능을 낸다. MDP를 풀었다는 것은 optimal value function을 찾았다는 뜻이다.

 

두 개의 policy를 비교할 수 있어야 optimal을 구할 수 있다.   


$$
\\
\pi \ge \pi' \ \ \ if \ \ \ v_{\pi}(s) \ge v_{\pi'}, \ \forall s
\\
$$


모든 state에 대해서 더 좋을 때 크기를 비교할 수 있고, 아래가 성립한다.   


$$
\\
\begin{align}
v_{\pi^*}(s) &= v_{*}(s) \\
q_{\pi^*}(s, a) &= q_{*}(s, a)
\end{align}
\\
$$

###    

### **1.4.3** **Optimal Policy**

Optimal policy는 $q_{\*}(s, a)$를 maximizing해서 찾을 수 있다.   


$$
\\
\pi_{*}(a | s) = 
\begin{cases}
1 & if \ \ \ a = \mathop{\arg \max}\limits_{a \in A} \ q_{*}(s, a) 
\\
0 & otherwise 
\end{cases}
\\
$$


$q_{\*}$ 는 $s$ 에서 어떤 $a$ 를 해야 하는지를 알려주기 때문에 당연히 optimal policy일 수밖에 없다. 그래서 $q_{*}$ 를 안다는 것은 문제를 푼 것과 같다.

 

*Bellman optimality Equation은 식에 max가 들어가기 때문에 nonlinear이다. 위의 방법처럼 직접적으로 풀 수 없기 때문에 approximation을 해야한다. 그 방법들은 Value Iteration, Policy Iteration, Q-learning, Sarsa 등이 있다.*