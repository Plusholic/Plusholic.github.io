---
layout: single
title:  "Value Function Approximation"
categories : 강화학습
tags : [python, reinforcement learning, 강화학습, Model Free control, On policy, SARSA, off policy, Q learning]
toc : true
author_profile : false
sidebar : 
    nav : "main"
use_math: true



---

# Introduction

table을 0으로 초기화해서 만들어 뒀었는데, state 갯수가 현실에서는 너무 많기 때문에 table을 만들어 둘 수 없다.

예를들어 Backgammon의 경우, $10^{20}$ 개의 state가 필요하다.

large MDP 에 대해서,  state value function $v$ 는 state 갯수만큼 빈칸이 있었고, action value function $q$ 는 state $\times$ action 만큼 빈칸이 있었다. 이는 메모리에 담을 수 없고, 담는다 하더라도 느릴 수밖에 없다. 그래서 다음과 같이 function approximation을 통해서 value function을 추정할 것이다.

$$
\hat{v}(s, w) \approx v_{\pi}(s) \\
\hat{q}(s, a, w) \approx q_{\pi}(s, a)
$$


function approximation은 함수처럼 생각할 수 있고, 그래서 잘 일반화되면 관찰하지 않은 state에 대해서도 값이 나오게 된다.

이제 우리는 MC나 TD learning 을 사용해서 parameter $w$ 를 업데이트 할 것이다.

![1](/images/강화학습_5/1.jpeg){: width="100%" height="100%"}

위의 그림과 같이, 어떤 blackbox에 들어가서 value function을 모방한 $\hat{v}(s, w), \hat{q}(s, a, w)$ 등이 나오게 된다. 

맨 오른쪽 그림처럼 action value function의 경우, 하나의 $s$ 에 대해 여러개의 아웃풋을 내도록 만드는것도 가능하다.

그래서 Function Approximatior는 어떤 함수를 쓰는것일까?

- Linear combination of features

- Neural Network

- Decision tree

  ...

등등이 가능하다. 우리는 여기서 미분가능한 function approximator인 Linear combination of features, Neural network를 다룰 것이다. 그리고 non-stationary, non-iid data에 대해서 다룰것이다.

# Increment Methods

## Gradient Descent

$J(w)$ 는 파라미터 $w$에 대해서 미분가능한 함수일 때, $J(w)$ 의 gradient는 다음과 같이 정의된다.
$$
\\
\nabla{w}J(w) = 
\begin{pmatrix} {\partial J(w) \over \partial w_{1}}
\\
\vdots
\\
{\partial J(w) \over \partial w_{1}} \end{pmatrix}
\\
$$
위의 식은 n개의 벡터로부터 방향을 알 수 있다. 즉, gradient는 방향을 나타낸다. 이 방향으로 조금씩 업데이트 해주면 $J(w)$ 가 최소가 되는 방향으로 이동하게 되는데, $J(w)$ 가 convex 하면 local minimum은 global minimum이 된다. 

![1](/images/강화학습_5/2.png){: width="100%" height="100%"}

우리는 $J$ 을 최소화하는 파라미터 $w$를 찾고 싶다.

$ -{1 \over 2}$ 는 미분할때 계산의 편의성을 위함이고, $\alpha$ 는 step size이다. learning rate라고 이해할 수 있다.


$$
\Delta w = -{1 \over 2} \alpha \nabla J(w)
$$


우리의 목표는 approximate value $\hat{v} (s, w)$ 와 true value $v_{\pi} (s)$ 사이의 mean squared error를 최소화시키는 파라미터 벡터 $w$ 를 찾는 것이다.

그러면 목적함수는 다음과 같다.
$$
\\
\begin{align}
J(w) &= \mathbb{E}_{\pi}[(v_{\pi}(S) - \hat{v} (S, w))^{2}] \\
\end{align}
\\
$$


이는 policy $\pi$를 따랐을 때의 $S$ 의 차이의 기댓값이다. 그리고 위의 식에 대입하고, 최소가 되는 w를 찾는 것이기 때문에 $w$에 대해 미분하여 정리하면 다음과 같다.


$$
\begin{align}
\Delta w &= -{1 \over 2} \alpha \nabla_{w} J(w) \\
&=\alpha \mathbb{E}_{\pi}[(v_{\pi}(S) - \hat{v} (S, w))\nabla{w} \ \hat{v}(S, w)] \\
\end{align}
$$


이는 $- \nabla_{w}$ 방향으로 ${1 \over 2}$ 만큼 업데이트해주는 것을 뜻한다. 이 과정을 반복하여 global minimum을 찾는다.

## Stochastic gradient descent

stochatic gradient descent는 위의 식에서 기댓값을 빼주는 방법이다. 기댓값이 아닌 sample 들을 input으로 넣어주는것인데, 많은 수의 반복이 수반된다면 결국 기댓값과 같아질 것이다.


$$
\Delta w =\alpha (v_{\pi}(S) - \hat{v} (S, w))\nabla{w} \ \hat{v}(S, w)
$$

## Linear Value Function Approximation

value function을 feature와의 linear combination으로 표현할 수 있다.


$$
\hat{v}(S, w) = x(S)^{T} w = \sum_{j=1}^{n} x_{j}(S)w_{j}
$$
여기서, feature value의 형태는 다음과 같다.


$$
x(s) =
\begin{pmatrix}
x_1(S) \\
\vdots \\
x_n(S)
\end{pmatrix}
$$
그러면 목적 함수는 다음과 같다.


$$
\\
\begin{align}
J(w) &= \mathbb{E}_{\pi}[(v_{\pi}(S) - x(S)^{T} w)^{2}] \\
\end{align}
\\
$$


Stochastic gradient descent는 global optimum으로 수렴한다. Linear value function이기 때문에, 감소하는 방향은 하나의 방향이기 때문이다. 그러면 위의 규칙에 따라서, 다음과 같이 업데이트 된다.


$$
\begin{align}
\nabla_{w} \hat{v} (S, w) &= x(S) \\
\Delta{w} &= \alpha(v_{\pi}(S) - \hat{v}(S, w))x(S)
\end{align}
$$


업데이트 = step size $\times$ prediction error $\times$ feature value

Table lookup의 경우, linear value function approximation의 한 경우라고 생각할 수 있는데, table lookup feature를 사용하면 다음과 같다.


$$
x^{table}(S) =
\begin{pmatrix}
1(S=s_{1}) \\
\vdots \\
1(S=s_{n})
\end{pmatrix}
$$
파라미터 벡터 $w$ 는 각 state 별로 value를 준다. 각 state마다 w를 1개 갖게 되고, 이는 lookup table에 n개의 요소가 있는 것과 같다.


$$
\hat{v}(S, w) =
\begin{pmatrix}
1(S=s_{1}) \\
\vdots \\
1(S=s_{n})
\end{pmatrix}
\cdot
\begin{pmatrix}
w_1 \\
\vdots \\
w_n
\end{pmatrix}
$$



지금까지 우리는 쉽게 하기 위해서 ture value function을 알고 있다고 가정하고 식을 전개하였다. 하지만 RL에서는 supervisor가 없고 단지 reward만 있을 뿐이다. 실제 value가 무엇이 될지는 모른다.

MC의 경우에, target은 return $G_{t}$


$$
\Delta{w} = \alpha({\color{red}{G_{t}}} - \hat{v}(S_t, w))\nabla_{w} \hat{v}(S_t, w)
$$


TD(0)의 경우에, target은 return $R_{t+1} + \gamma \hat{v} (S_{t+1}, w)$


$$
\Delta{w} = \alpha({\color{red}{R_{t+1}} + \gamma \hat{v}(S_{t+1}, w)} - \hat{v}(S_t, w))\nabla_{w} \hat{v}(S_t, w)
$$


TD(\lambda)의 경우에, target은 \lambda - return $G_{t}^{\lambda}$


$$
\Delta{w} = \alpha({\color{red}{G_{t}^{\lambda}}} - \hat{v}(S_t, w))\nabla_{w} \hat{v}(S_t, w)
$$


### Monte-Carlo with Value Function Approximation

MC의 경우에, $G_{t}$ 는 discounted cumulative sum 이기 때문에, value function의 unbiased estimate가 된다. supervised learning으로 생각하면 training data가 된다.


$$
<S_1, G_1>, <S_2, G_2>, \cdots, <S_T, G_T>
$$


예를들어서, linear Monte-Carlo policy evaluation을 사용하면


$$
\begin{align}
\Delta{w} &= \alpha({\color{red}{G_{t}}} - \hat{v}(S_t, w))\nabla_{w} \hat{v}(S_t, w) \\
&= \alpha (G_t - \hat{v}(S_t, w))x(S_t)
\end{align}
$$


Monte-Carlo evaluation은 local optimum으로 수렴하게 된다. non-linear일 때는 수렴 조건이 훨씬 까다로워지지만, Monte-Carlo evaluation은 이 때도 수렴한다.

### TD Learning with Value Function Approximation



TD-target $R_{t+1} + \gamma \hat{v} (S_{t+1}, w)$ 는 true value의 biased 샘플이라는 것을 알고 있다.

위에 했던 것 처럼, training data로서 생각해본다면


$$
<S_1, R_2+\gamma \hat{v} (S_2, w)>, <S_2, R_3+\gamma \hat{v} (S_3, w)> , \cdots, <S_{T-1}, R_T>
$$


예를 들어서, linear TD(0)를 사용한다면


$$
\begin{align}
\Delta{w} &= \alpha({\color{red}{R_{t+1}} + \gamma \hat{v}(S_{t+1}, w)} - \hat{v}(S_t, w))\nabla_{w} \hat{v}(S_t, w) \\
&= \alpha \delta x(S)

\end{align}
$$


여기서, 빨간색 부분은 위에서 $w$를 상수 취급하고 미분한 자리인데, $w$에 대한 함수를 넣어도 되는가? 라는 의문이 생긴다. 강의에서는 한 스텝을 경험한게 정확하다고 가정하고 업데이트를 해주는데, 이것도 업데이트를 하면 한 스텝을 더 가서 과거를 본 것과, 과거에서 한 스텝 이후를 본 것이 섞이게 된다고 한다. ( 직관적으로는 알겠는데, 자세히는 모르겠음. 알아내면 업데이트 추가 예정)

Linear TD(0)도 global optimum으로 거의 수렴한다.

### TD($\lambda$) with Value Function Approximation



$\lambda$-return인 $G_t^{\lambda}$ 또한 true value의 biased 샘플이라는 것을 알고 있다.

위와 동일하게 training data로서 적용해보면


$$
<S_1, G_1^\lambda>,<S_2, G_2^\lambda>,\cdots,<S_{T-1}, G_{T-1}^\lambda>
$$


Forward view linear TD($\lambda$)


$$
\begin{align}
\Delta w &= \alpha(G_t^\lambda - \hat{v}(S_t, w))\nabla_w \hat{v}(S_t, w) \\
&= \alpha(G_t^\lambda - \hat{v}(S_t, w))x(S_t)
\end{align}
$$


Backward view linear TD($\lambda$)


$$
\begin{align}
\delta_t &= R_{t+1} + \gamma \hat{v} (S_{t+1}, w) - \hat{v}(S_t, w) \\
E_t &= \gamma \lambda E_{t-1} + x(S_t) \\
\Delta w &= \alpha \delta_t E_t
\end{align}
$$


## Control with Value Function Approximation

지금까지는 value function 을 학습하는 predict 였는데, 이제부터는 control을 알아보고자 한다.

Policy evaluation \textcolor{red}{Approximatie} policy evaluation, $\hat{q}(\cdot, \cdot, w) \approx q_{\pi}$

Policy improvement : $epsilon$-greedy policy improvement

### Action Value Function Approximation

v 자리에 q가 들어가는것만 다를 뿐, 위와 똑같다. approximate action value function인 $\hat{q} (S, A, w)$ 와 true action value function $q_{\pi}(S, A)$ MSE를 최소화한다.
$$
\\
\begin{align}
J(w) &= \mathbb{E}_{\pi}[(q_{\pi}(S, A) - \hat{q} (S, A, w))^{2}] \\
\end{align}
\\
$$


그리고 stochastic gradient descent 를 사용해서 local mimimum을 찾으면 다음과 같다.


$$
\begin{align}
-{1 \over 2} \nabla_{w} J(w) &= (q_{\pi}(S, A) - \hat{q}(S, A, w))\nabla_w \hat{q}(S, A, w)
\\
\Delta w &= \alpha(q_{\pi}(S, A) - \hat{q}(S, A, w))\nabla_w \hat{q}(S, A, w)
\end{align}
$$

### Linear Action-Value Function Approximation

action value function을 feature와의 linear combination으로 표현할 수 있다.


$$
\hat{q}(S, A, w) = x(S, A)^{T} w = \sum_{j=1}^{n} x_{j}(S, A)w_{j}
$$


여기서, feature value의 형태는 다음과 같다.


$$
x(S, A) =
\begin{pmatrix}
x_1(S, A) \\
\vdots \\
x_n(S, A)
\end{pmatrix}
$$


Stochastic gradient descent는 다음과 같이 업데이트 된다.


$$
\begin{align}
\nabla_{w} \hat{q} (S, A, w) &= x(S, A) \\
\Delta{w} &= \alpha(q_{\pi}(S, A) - \hat{a}(S, A, w))x(S, A)
\end{align}
$$


Prediction과 같이, traget $q_{\pi}(S, A)$를 대체할 수 있다.

MC traget은 return $G_t$


$$
\Delta w= \alpha (G_t - \hat{q}(S_t, A_t, w))\nabla_w \hat{q}(S_t, A_t, w)
$$


TD(0) traget은 return $R_{t+1} + \gamma Q(S_{t+1}, A_{t+1})$


$$
\Delta w= \alpha (R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}, w) - \hat{q}(S_t, A_t, w))\nabla_w \hat{q}(S_t, A_t, w)
$$


forward view TD($\lambda$) target은 action-value $\lambda$-return


$$
\Delta w= \alpha (q_t^{\lambda} - \hat{q}(S_t, A_t, w))\nabla_w \hat{q}(S_t, A_t, w)
$$


backward view TD($\lambda$) target은 다음과 같이 업데이트 된다.


$$
\begin{align}
\delta_t &= R_{t+1} + \gamma \hat{q} (S_{t+1}, A_{t+1}, w) - \hat{q}(S_t, A_t, w) \\
E_t &= \gamma \lambda E_{t-1} + \nabla w \hat{q} (S_t, A_t, w) \\
\Delta w &= \alpha \delta_t E_t
\end{align}
$$


## Convergence of Algorithms

![1](/images/강화학습_5/3.png){: width="100%" height="100%"}

![1](/images/강화학습_5/4.png){: width="100%" height="100%"}

# Batch Method

<2022.09.09업데이트 중>





Refference

1. https://www.davidsilver.uk/teaching/
2. 팡요랩 강화학습 https://www.youtube.com/watch?v=71nH1BUjhNw
3. 바닥부터 시작하는 강화학습





