---
layout: single
title:  "Value Function Approximation"
categories : 강화학습
tags : [python, reinforcement learning, 강화학습, Model Free control, On policy, SARSA, off policy, Q learning]
toc : true
author_profile : false
sidebar : 
    nav : "main"
use_math: true



---

# Value Function Approximation



## Introduction

table을 0으로 초기화해서 만들어 뒀었는데, state 갯수가 현실에서는 너무 많기 때문에 table을 만들어 둘 수 없다.

예를들어 Backgammon의 경우, $10^{20}& 개의 state가 필요하다.

large MDP 에 대해서, 다음과 같은 문제점이 있다. \\

state value function $v$ 는 state 갯수만큼 빈칸이 있었고 \\

action value function $q$ 는 state $\times$ action 만큼 빈칸이 있었다. \\

이는 메모리에 담을 수 없고, 담는다 하더라도 느릴 수밖에 없다.

그래서 다음과 같이 function approximation을 통해서 value function을 추정할 것이다.
$$
\hat{v}(s, w) \approx v_{\pi}(s) \\
\hat{q}(s, a, w) \approx q_{\pi}(s, a)
$$


function approximation은 함수처럼 생각할 수 있고, 그래서 잘 일반화되면 관찰하지 않은 state에 대해서도 값이 나오게 된다.

이제 우리는 MC나 TD learning 을 사용해서 parameter $w$ 를 업데이트 할 것이다.

![1](/images/강화학습_5/1.jpeg)

위의 그림과 같이, 어떤 blackbox에 들어가서 value function을 모방한 $\hat{v}(s, w), \hat{q}(s, a, w)$ 등이 나오게 된다. 

맨 오른쪽 그림처럼 action value function의 경우, 하나의 $s$ 에 대해 여러개의 아웃풋을 내도록 만드는것도 가능하다.

그래서 Function Approximatior는 어떤 함수를 쓰는것일까?

- Linear combination of features

- Neural Network

- Decision tree

  ...

등등이 가능하다. 우리는 여기서 미분가능한 function approximator인 Linear combination of features, Neural network를 다룰 것이다. 그리고 non-stationary, non-iid data에 대해서 다룰것이다.



## Increment Methods



### Gradient Descent

$J(w)$ 는 파라미터 $w$에 대해서 미분가능한 함수일 때, $J(w)$ 의 gradient는 다음과 같이 정의된다.
$$
\nabla{w}J(w) = 
\begin{pmatrix} {\partial J(w) \over \partial w_{1}}
\\
\vdots
\\
{\partial J(w) \over \partial w_{1}} \end{pmatrix}
$$
위의 식은 n개의 벡터로부터 방향을 알 수 있다. 즉, gradient는 방향을 나타낸다. 이 방향으로 조금씩 업데이트 해주면 $J(w)$ 가 최소가 되는 방향으로 이동하게 되는데, $J(w)$ 가 convex 하면 local minimum은 global minimum이 된다. 

우리는 $J$ 을 최소화하는 파라미터 $w$를 찾고 싶기 때문에, 

$ -{1 \over 2}$ 는 미분할때 계산의 편의성을 위함이고, $\alpha$ 는 step size이다. learning rate라고 이해할 수 있다.
$$
\Delta w = -{1 \over 2} \alpha \nabla J(w)
$$
우리의 목표는 approximate value $\hat{v} (s, w)$ 와 true value $v_{\pi} (s)$ 사이의 mean squared error를 최소화시키는 파라미터 벡터 $w$ 를 찾는 것이다.

그러면 목적함수는 다음과 같다.
$$
\\
\begin{align}
J(w) &= \mathbb{E}_{\pi}[(v_{\pi}(S) - \hat{v} (S, w))^{2}] \\
\end{align}
\\
$$
이는 policy $\pi$를 따랐을 때의 $S$ 의 차이의 기댓값이다.

그리고 이를 위의 식에 대입하고, 최소가 되는 w를 찾는 것이기 때문에 $w$에 대해 미분하여 정리하면 다음과 같다.
$$
\begin{align}
\Delta w &= -{1 \over 2} \alpha \nabla_{w} J(w) \\
&=\alpha \mathbb{E}_{\pi}[(v_{\pi}(S) - \hat{v} (S, w))\nabla{w} \ \hat{v}(S, w)] \\
\end{align}
$$
이는 $- \nabla_{w}$ 방향으로 ${1 \over 2}$ 만큼 업데이트해주는 것을 뜻한다. 이 과정을 반복하여 global minimum을 찾는다.



### Stochastic gradient descent

stochatic gradient descent는 위의 식에서 기댓값을 빼주는 방법이다. 기댓값이 아닌 sample 들을 input으로 넣어주는것인데, 많은 수의 반복이 수반된다면 결국 기댓값과 같아질 것이다.
$$
\Delta w =\alpha (v_{\pi}(S) - \hat{v} (S, w))\nabla{w} \ \hat{v}(S, w)
$$

### Linear Value Function Approximation

value function을 feature와의 linear combination으로 표현할 수 있다.
$$
\hat{v}(S, w) = x(S)^{T} w = \sum_{j=1}^{n} x_{j}(S)w_{j}
$$
여기서, feature value의 형태는 다음과 같다.
$$
x(s) =
\begin{pmatrix}
x_1(S) \\
\vdots \\
x_n(S)
\end{pmatrix}
$$
그러면 목적 함수는 다음과 같다.
$$
\\
\begin{align}
J(w) &= \mathbb{E}_{\pi}[(v_{\pi}(S) - x(S)^{T} w)^{2}] \\
\end{align}
\\
$$
Stochastic gradient descent는 global optimum으로 수렴한다. Linear value function이기 때문에, 감소하는 방향은 하나의 방향이기 때문이다. 그러면 위의 규칙에 따라서, 다음과 같이 업데이트 된다.
$$
\begin{align}
\nabla_{w} \hat{v} (S, w) &= x(S) \\
\Delta{w} &= \alpha(v_{\pi}(S) - \hat{v}(S, w))x(S)
\end{align}
$$
업데이트 = step size $\times$ prediction error $\times$ feature value

Table lookup의 경우, linear value function approximation의 한 경우라고 생각할 수 있는데, table lookup feature를 사용하면 다음과 같다.
$$
x^{table}(S) =
\begin{pmatrix}
1(S=s_{1}) \\
\vdots \\
1(S=s_{n})
\end{pmatrix}
$$
파라미터 벡터 $w$ 는 각 state 별로 value를 준다. 각 state마다 w를 1개 갖게 되고, 이는 lookup table에 n개의 요소가 있는 것과 같다.
$$
\hat{v}(S, w) =
\begin{pmatrix}
1(S=s_{1}) \\
\vdots \\
1(S=s_{n})
\end{pmatrix}
\cdot
\begin{pmatrix}
w_1 \\
\vdots \\
w_n
\end{pmatrix}
$$


 

<2022.09.07업데이트 중>





Refference

1. https://www.davidsilver.uk/teaching/
2. 팡요랩 강화학습 https://www.youtube.com/watch?v=71nH1BUjhNw
3. 바닥부터 시작하는 강화학습





