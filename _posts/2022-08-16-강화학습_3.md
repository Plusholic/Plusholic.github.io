---
layout: single
title:  "강화학습_3"
categories : coding
tags : [python, reinforcement learning, 강화학습, dynamic programming, Policy iteration, Value iteration]
toc : true
author_profile : false
sidebar : 
    nav : "docs"
use_math: true


---



# **3**   **Model Free Prediction**

## **3.1**   **Monte Carlo Learning**

Monte Carlo Learning은 경험으로 직접 배운다는 뜻이다. MC는 model free이고, MDP의 정보가 없다. 그래서 MC는 bootstrapping을 하지 않고 episode를 완결시켜서 학습한다(모든 episode는 terminate 되어야한다). MC는 value = mean return이라는 가장 간단한 아이디어를 사용한다.

 

### **3.1.1** **Monte Carlo Policy Evaluation**

MC Policy Evaluation의 목적은 policy $\pi$ 에서의 episode들의 $v_{\pi}$ 를 학습하는 것이고, expected return 대신 empirical mean return을 사용한다.

하나의 state를 여러 번 방문할 수 있는데, 이것을 다 고려하는 것을 every visit이라고 하고, 한 번의 방문만을 고려하는 것은 first visit이라고 한다.

Value는 mean return에 의해서 estimate 된다. $V(s) = S(s) / N(s)$ (리턴의 합 / 방문횟수)

우리의 목적은 모든 state를 평가하는 것이기 때문에, Policy는 골고루 움직여서 모든 state를 방문한다고 가정한다. First visit과 every visit의 결과는 law of large number 때문에 같다. ($V(s) \rightarrow v_{\pi}(s)$ as $N(s) \rightarrow \infinite$)

 

### **3.1.2** **Incremental Monte Carlo Updates**

$V(s)$를 episode $S_{1}, A_{1}, R_{2}, \cdots, S_{T}$ 이후 incrementally 하게 업데이트한다. 각각의 state $S_{t}$와 return $G_{t}$ 에 대해서
$$
\begin{align}
N(S_{t}) &\leftarrow N(S_{t}) + 1 \\
V(S_{t}) &\leftarrow V(S_{t}) + \frac{1}{N(S_t)}(G_{t} - V(S_{t})) \\
V(S_{t}) &\leftarrow V(S_{t}) + \alpha (G_{t} - V(S_{t}))
\end{align}
$$


Non stationary problems(time에 따라서 변하는 문제)에서, old episode는 잊어버리고, 매번 $G$ 를 향하도록 $V$ 를 조금씩 업데이트 해준다.

 

## **3.2**   **Temporal Difference Learning(TD)**

TD method 는 experience의 episode로부터 직접적으로 학습한다. 그렇기 때문에 model free이고, MDP의 정보가 필요하지 않다. TD는 episode가 끝나지 않아도 bootstrapping을 통해서 업데이트 할 수 있고, TD는 guess로 guess를 업데이트 해준다.

*TD의 목적은 policy* $\pi$ *의* *experience에서* $v_{\pi}$ *를 학습하는 것이다.*

 

### **3.2.1** **TD(0)**

Simplest TD leaning algorithm인 TD(0)는 value $V(S_{t})$ 를 toward estimated return $R_{t+1} + \gamma V(S_{t+1})$ 으로 업데이트 해준다.
$$
V(S_{t}) \leftarrow V(S_{t}) + \alpha (R_{t+1} + \gamma V(S_{t+1}) - V(S_{t}))
$$
$R_{t+1} + \gamma V_(S_{t+1})$는 TD target이라고 부르고, $\delta_{t} = R_{t+1} + \gamma V(S_{t+1}) - V(S_{t})$ 를 TD error라고 부른다.

MC는 실현된 값으로 업데이트 해주지만 TD는 다음 스텝의 예측치로 현재 예측치를 업데이트 해준다.

TD는 final outcome을 알기 전에 학습할 수 있다.

TD는 every step마다 실시간으로 학습할 수 있지만 MC는 episode가 끝날 때까지 기다려야 return을 알 수 있다.

TD는 final outcome 없이 학습할 수 있다. 즉, nonterminating env 에서도 학습할 수 있다. 하지만 MC는 terminating env에서만 학습이 가능하다.

Return $G_{t} = R_{t+1} + \gamma R_{t+2} + \cdots + R_{T}$ 는 $v_{\pi}(S_{t})$의 unbiased estimate 이다. 하지만 모든걸 더해서 variance가 높다.

True TD target $R_{t+1} + \gamma V(S_{t+1})$ 은 $v_{\pi}(S_{t})$ 의 unbiased estimate 이다.

TD target은 1 step 이기 때문에, return 보다 낮은 variance를 갖고, 높은 bias를 가진다. 그래서 bias 관점에서는 TD보다 MC가 좋다.

MC는 높은 variance를 가지지만 zero bias를 가진다.

수렴성이 좋고 초기값에 민감하지 않다. 이해와 사용이 간단하다.

TD는 낮은 variance를 가지지만 약간의 bias가 있다.

보통 MC보다 효율적이고 TD(0)는 $v_{\pi}(s)$ 로 수렴하지만 초기값에 민감하다.

 

### **3.2.2** **Convergence**

MC와 TD는 에피소드를 무한번 뽑으면 수렴한다. $V(s) \rightarrow v_{\pi}(s)$ as experience $\rightarrow \infinite$ 그러나 유한개의 에피소드가 있을 때 MC와 TD는 어떤 값으로 수렴하는가?

MC는 mean squared error가 최소가 되는 값으로 수렴한다.

$$
\sum_{k=1}^{K} \sum_{t=1}^{T_{k}} (G_{t}^{k} - V(s_{t}^{k}))^{2}
$$


TD(0)는 maximum likelihood Markov model로 수렴한다.

MDP의 solution $<S, A, \widehat{P}, \widehat{R}, \gamma>$ 에서, 

$$
\begin{align}
\widehat{P}_{ss'}^a &= \frac{1}{N(s, a)} \sum_{k=1}^{K} \sum_{t=1}^{T_k} 1 (s_{t}^{k}, a_{t}^{k}, s_{t+1}^{k} = s, a, s') \\
\widehat{R}_{ss'}^a &= \frac{1}{N(s, a)} \sum_{k=1}^{K} \sum_{t=1}^{T_k} 1 (s_{t}^{k}, a_{t}^{k} = s, a)r_{t}^{k} \\
\end{align}
$$


이다.

### **3.2.3** **Difference of TD and MC**

TD는 Markov property를 이용하여 추측하지만 MC는 Markov property를 이용하지 않고 Mean square error를 minimize하여 추측한다.

Bootstrapping은 depth, Sapling은 width로 볼 수 있다.

MC는 bootstrap이 불가능하며 DP는 samppling이 불가능하다.

$$
\begin{align}

n &= 1 \ \ (TD) \ \ G_{t}^{(1)} = R_{t+1} + \gamma V(S_{t+1}) \\
n &= 2 \ \ G_{t}^{(2)} = R_{t+1} + \gamma R_{t+2} + \gamma^{2} V(S_{t+2}) \\

\cdots \\

n &= \infin \ \ (MC) \ \ G_{t}^{(\infin)} = R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{T-1} R_{T}

\end{align}
$$
 

### **3.2.4** $n-step TD(\lambda)$

n-step return은 다음과 같이 정의한다.

$$
G_{t}^{(n)} = R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1} R_{t+n} + \gamma^{n} V(S_{t+n})
$$


그리고, n-step TD learning은 다음과 같다.

$$
V(S_{t}) \leftarrow V(S_{t}) + \alpha(G_{t}^{(n)} - V(S_{t}))
$$


$TD(\lambda)$ 의 $\lambda - return G_{t}^{\lambda}$ 는 weight $(1-\lambda) lambda^{n-1}$ 을 사용해서 geometric series의 sum으로 나타낸다.

$$
G_{t}^{\lambda} = (1 - \lambda) \sum_{n=1}^{\infin} \lambda^{n-1} G_{t}^{(n)}
$$
 

### **3.2.5** **Forward view and Backward view** $TD(\lambda)$

Forward view $TD(\lambda)$ 는 다음과 같다.

$$
V(S_{t}) \leftarrow V(S_{t}) + \alpha (G_{t}^{\lambda} - V(S_{t}))
$$
Backward view $TD(\lambda)$ 는 Forward $TD(\lambda)$ 의 장점과 $TD(0)$ 의 장점을 모두 가지고 있다. TD error $\delta_{t}$ 와 eligibility trace ![img](file:////Users/jeonjunhwi/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image095.png)에 대해서 다음과 같다.

![img](file:////Users/jeonjunhwi/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image097.png)



![img](file:////Users/jeonjunhwi/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image099.png)

Eligibility trace는 다음과 같다.

![img](file:////Users/jeonjunhwi/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image101.png)



![img](file:////Users/jeonjunhwi/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image103.png)

![img](file:////Users/jeonjunhwi/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image105.png)이라면 단지 현재의 state만 업데이트 한다.

![img](file:////Users/jeonjunhwi/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image107.png)

![img](file:////Users/jeonjunhwi/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image109.png)

이것은 TD(0)의 업데이트와 동일하다.

![img](file:////Users/jeonjunhwi/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image111.png)

![img](file:////Users/jeonjunhwi/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image113.png)은 MC의 total update와 동일하다.

Offline update의 합은 forward-view와 backward-view ![img](file:////Users/jeonjunhwi/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image076.png)로 구별된다.

![img](file:////Users/jeonjunhwi/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image116.png)









