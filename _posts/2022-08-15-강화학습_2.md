---
layout: single
title:  "강화학습_2"
categories : 강화학습
tags : [python, reinforcement learning, 강화학습, dynamic programming, Policy iteration, Value iteration]
toc : true
author_profile : false
sidebar : 
    nav : "docs"
use_math: true

---

 

# **2  Planning by Danamic Programming**

Planning은 environment에 대한 model이 있을 때 더 나은 plan을 찾아가는 과정이다. Dynamic programming은 큰 문제를 한번에 풀기 어려우니 작은 subproblem으로 나누고 작은 문제에 대해 solution을 모아서 큰 문제를 푸는 것을 말한다.

Model based의 강화학습에 쓰이는 방법이 planning이다. 즉 Planning은 full knowledge 일 때 MDP를 푸는 것이다.

Planning은 목적에 따라서 Prediction과 Control로 나뉜다.

 

## **2.1**   **Prediction and Control**

Prediction은 MDP, MRP에서 Policy가 있을 때 value function을 찾는 것이다. 즉, Input으로 MDP $<S, A, P, R, \gamma>$ 와 policy $\pi$ 또는 MRP $<S, P^{\pi}, R^{\pi}, \gamma>$ 가 주어졌을 때 value fuction $v_{\pi}$ 를 찾는 것이다.

Control은 Input으로 MDP $<S, A, P, R, \gamma>$ 가 주어졌을 떄, Output으로 optimal value function $v_{\*}$ 그리고 Optimal policy $\pi_{\*}$ 를 찾는 것이다.

 

## **2.2**   **Iterative Policy Evaluation**

Iterative Policy Evaluation은 Policy를 따랐을 때 return을 얼마 받는지를 찾는 것(value function을 찾는 문제) 이다.   


$$
v^{k+1} = R^{\pi} + \gamma P^{\pi} v^{k}
$$


Policy $\pi$ 가 주어졌을 때, Evalutaion과 Improve를 할 수 있다.

Evalutaion : Policy $\pi$ 를 평가한다.  


$$
v_{\pi}(s) = \mathbb{E}[R_{t+1} + \gamma R_{t+2} + \cdots | S_{t} = s]
$$


Improve : Policy를 향상시키기 위해서 $v_{\pi}$ 에 대해서 greedy하게 action을 취한다.  


$$
\pi' = greedy(v_{\pi})
$$


이는 Policy $\pi$ 가 주어졌을 때, Policy를 평가만 해도 그 중에서 greedy 하게 움직이면 optimal을 얻을 수 있다는 뜻이다.

Small gridworld에서 improved policy는 optimal이고, $\pi' = \pi^{\*}$ 이고, Policy iteration의 과정은 언제나 $\pi^{\*}$ 로 수렴한다. Evaluation과 improve를 반복하면 optimal에 도달한다.

Deterministic policy $a = \pi(s)$ 를 고려했을 때 우리는 greedy한 action을 통해서 policy를 향상시킬 수 있다.   


$$
\begin{align}
\pi'(s) &= \mathop{\arg \max}\limits_{a \in A} \ q_{\pi}(s,a) \\
q_{\pi}(s, \pi'(s)) &= \max\limits_{a \in A} {q_{\pi}(s, a) \ge q_{\pi} (s, \pi(s)) = v_{\pi}(s)} \\
v_{\pi}(s) & \le q_{\pi}(s, \pi(s)) = \mathbb{E}_{\pi'}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_{t} = s] \\
& \le \mathbb{E}_{\pi'}[R_{t+1} + \gamma q_{\pi}(S_{t+1}, \pi(S_{t+1})) | S_{t} = s] \\
& \le \mathbb{E}_{\pi'}[R_{t+1} + \gamma R_{t+2} + \gamma^2 q_{\pi}(S_{t+2}, \pi'^{S_{t+2}}) | S_t = s] \\
& \le \mathbb{E}_{\pi'} [R_{t+1} + \gamma R_{t+2} + \cdots | S_t = s] = v_{\pi'}(s)
\end{align}
$$


만약 improvement가 멈춘다면,   


$$
q_{\pi}(s, \pi'(s)) = \max\limits_{a \in A} q_{\pi}(s, a) = q_{\pi}(s, \pi(s)) = v_{\pi}(s)
$$


이고, Bellman optimality equation에 의해 아래의 식이 만족된다.   


$$
v_{\pi} = \max\limits_{a \in A} q_{\pi}(s, a)
$$


그러므로 $v_{\pi}(s) = v_{\*}(s)$이며, $\pi$ 는 optimal policy이다.

 

## **2.3**   **Value iteration**

Value iteration은 만약 우리가 subproblem $v_{\*}(s)$ 의 solution을 알고 있다면 policy 없이 value만 iteration 해서 update해도 optimal을 달성할 수 있다는 것이다.   


$$
v_{*}(s) \leftarrow \max\limits_{a \in A} (R_s^a + \gamma \sum_{s' \in S} P_{ss'}^{a} v_{*}(s'))
$$

